import numpy as np
import pandas as pd
import logging
import os
import joblib
import warnings
import matplotlib.pyplot as plt
import requests
from config.settings import TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID, ML_CONFIG

# TensorFlow ë¡œê·¸ ë ˆë²¨ ì¡°ì •
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings("ignore", category=UserWarning)

try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import LSTM, Dense, Dropout
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
    from tensorflow.keras.regularizers import l2
    HAS_TF = True
except ImportError:
    HAS_TF = False

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler

logger = logging.getLogger(__name__)

class MLPredictor:
    def __init__(self, lookback_window=60, model_type="random_forest"):
        self.lookback_window = lookback_window
        self.model_type = model_type
        self.model = None
        self.scaler = MinMaxScaler()
        self.is_trained = False
        
    def _build_lstm_model(self, input_shape):
        """
        [Request] ì‹œê³„ì—´ ì˜ˆì¸¡ì— ìµœì í™”ëœ ê³ ì„±ëŠ¥ LSTM ëª¨ë¸ êµ¬ì¶•
        1. ë‹¤ì¸µ LSTM êµ¬ì¡° (Stacked LSTM)
        2. ê·œì œí™” (Dropout + L2 Regularization)
        3. í•™ìŠµ ìµœì í™” (Adam + MSE)
        4. ë°ì´í„° ì‰ì´í”„ ìë™ ì¡°ì • (input_shape ì¸ì í™œìš©)
        """
        if not HAS_TF:
            logger.error("TensorFlow not installed.")
            return None

        model = Sequential()
        
        # 1. ë‹¤ì¸µ LSTM êµ¬ì¡° (Stacked LSTM) - ì²« ë²ˆì§¸ ë ˆì´ì–´
        # return_sequences=True: ë‹¤ìŒ LSTM ë ˆì´ì–´ë¡œ ì‹œí€€ìŠ¤ ì „ë‹¬
        model.add(LSTM(64, return_sequences=True, input_shape=input_shape,
                       kernel_regularizer=l2(0.001))) # 2. L2 ê·œì œí™”
        model.add(Dropout(0.2)) # 2. Dropout
        
        # ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´
        model.add(LSTM(32, return_sequences=False,
                       kernel_regularizer=l2(0.001)))
        model.add(Dropout(0.2))
        
        # ì¶œë ¥ì¸µ (ë‹¤ìŒ ì¢…ê°€ ì˜ˆì¸¡ - Regression)
        model.add(Dense(1))
        
        # 3. í•™ìŠµ ìµœì í™” ì„¤ì •
        # Optimizer: Adam (lr=0.001)
        # Loss: MSE (Mean Squared Error)
        optimizer = Adam(learning_rate=0.001)
        model.compile(optimizer=optimizer, loss='mse')
        
        return model

    def prepare_data(self, data):
        """ë°ì´í„° ì „ì²˜ë¦¬ (Scaling + Windowing)"""
        if len(data) < self.lookback_window + 1:
            return None, None

        # ì¢…ê°€ ê¸°ì¤€ ì˜ˆì¸¡
        close_prices = data['close'].values.reshape(-1, 1)
        
        # Scaling
        if not self.is_trained:
            scaled_data = self.scaler.fit_transform(close_prices)
        else:
            scaled_data = self.scaler.transform(close_prices)
        
        X, y = [], []
        for i in range(self.lookback_window, len(scaled_data)):
            X.append(scaled_data[i-self.lookback_window:i, 0])
            y.append(scaled_data[i, 0])
            
        X, y = np.array(X), np.array(y)
        
        if self.model_type == "lstm":
            # LSTM input shape: (samples, time steps, features)
            X = np.reshape(X, (X.shape[0], X.shape[1], 1))
            
        return X, y

    def train(self, data, epochs=5, batch_size=64, **kwargs):
        """ëª¨ë¸ í•™ìŠµ"""
        try:
            X, y = self.prepare_data(data)
            if X is None or len(X) == 0:
                logger.warning("í•™ìŠµ ë°ì´í„° ë¶€ì¡± ")
                return

            if self.model_type == "lstm":
                if not HAS_TF:
                    logger.error("TensorFlow ë¯¸ì„¤ì¹˜ë¡œ LSTM í•™ìŠµ ë¶ˆê°€")
                    return
                logger.error("TensorFlowë¡œ í•™     ìŠµì‹œì‘")
                # 4. ë°ì´í„° ì‰ì´í”„ ìë™ ì¡°ì •
                input_shape = (X.shape[1], 1)
                self.model = self._build_lstm_model(input_shape)
                
                # [New] ëª¨ë¸ êµ¬ì¡° ìš”ì•½ ë¡œê·¸ ì¶œë ¥
                self.model.summary(print_fn=logger.info)
                
                # 3. í•™ìŠµ ìµœì í™” ì„¤ì • (ReduceLROnPlateau)
                # í•™ìŠµ ì •ì²´ ì‹œ í•™ìŠµë¥  ìë™ ê°ì†Œ
                reduce_lr = ReduceLROnPlateau(
                    monitor='loss', 
                    factor=0.5, 
                    patience=5, 
                    min_lr=0.00001,
                    verbose=0
                )

                # [New] EarlyStopping (ê³¼ì í•© ë°©ì§€)
                early_stopping = EarlyStopping(
                    monitor='val_loss',
                    patience=3, # [Request] í•™ìŠµ ì •ì²´ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨ (10 -> 3)
                    restore_best_weights=True,
                    verbose=0
                )
                
                val_split = ML_CONFIG.get("validation_ratio", 0.1)
                
                self.model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=0, 
                               validation_split=val_split, callbacks=[reduce_lr, early_stopping])
                
            else: # random_forest (Classifier)
                logger.error("Trandom_forestë¡œ í•™ìŠµì‹œì‘")
                # RFëŠ” ë°©í–¥ì„±(0/1) ì˜ˆì¸¡ìœ¼ë¡œ ë³€í™˜ í•„ìš”
                y_class = (y > X[:, -1]).astype(int)
                
                self.model = RandomForestClassifier(n_estimators=100, random_state=42)
                self.model.fit(X, y_class)
                
            self.is_trained = True
            
            # [New] í•™ìŠµ í›„ í‰ê°€ ì‹¤í–‰
            self.evaluate(data)
            
            # [New] ì‹œê°í™” ë° í…”ë ˆê·¸ë¨ ì „ì†¡ (LSTMì¸ ê²½ìš°)
            if self.model_type == "lstm":
                self._visualize_and_send(X, y)
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")

    def evaluate(self, data):
        """[New] ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (Loss, Accuracy)"""
        if not self.is_trained or self.model is None:
            return

        try:
            X, y = self.prepare_data(data)
            if X is None or len(X) == 0:
                return

            if self.model_type == "lstm":
                # 1. Loss (MSE)
                loss = self.model.evaluate(X, y, verbose=0)
                
                # 2. Directional Accuracy (ë°©í–¥ì„± ì •í™•ë„)
                # ì˜ˆì¸¡ê°’ ìƒì„±
                pred_scaled = self.model.predict(X, verbose=0)
                
                # ì§ì „ ê°€ê²© (Xì˜ ë§ˆì§€ë§‰ ìŠ¤í… ê°’)
                prev_prices = X[:, -1, 0]
                
                # ì‹¤ì œ ë°©í–¥: y > prev_prices
                actual_dir = (y > prev_prices).astype(int)
                # ì˜ˆì¸¡ ë°©í–¥: pred > prev_prices
                pred_dir = (pred_scaled.flatten() > prev_prices).astype(int)
                
                accuracy = np.mean(actual_dir == pred_dir)
                
                logger.info(f"ğŸ“Š [LSTM í‰ê°€] Loss(MSE): {loss:.6f} | ë°©í–¥ì„± ì •í™•ë„: {accuracy*100:.2f}%")
                
            else: # random_forest
                y_class = (y > X[:, -1]).astype(int)
                accuracy = self.model.score(X, y_class)
                logger.info(f"ğŸ“Š [RF í‰ê°€] Accuracy: {accuracy*100:.2f}%")
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")

    def _visualize_and_send(self, X, y):
        """[New] í•™ìŠµ ê²°ê³¼ ì‹œê°í™” ë° í…”ë ˆê·¸ë¨ ì „ì†¡"""
        try:
            # Headless ëª¨ë“œ ì„¤ì • (ì„œë²„ í™˜ê²½ í˜¸í™˜)
            plt.switch_backend('Agg')
            
            # ì˜ˆì¸¡
            pred_scaled = self.model.predict(X, verbose=0)
            
            # ì—­ë³€í™˜ (ìŠ¤ì¼€ì¼ë§ ë³µêµ¬)
            real_y = self.scaler.inverse_transform(y.reshape(-1, 1))
            real_pred = self.scaler.inverse_transform(pred_scaled)
            
            # ê·¸ë˜í”„ ìƒì„±
            plt.figure(figsize=(10, 5))
            
            # ìµœê·¼ 100ê°œë§Œ í‘œì‹œ (ê°€ë…ì„±)
            display_len = min(len(real_y), 100)
            
            plt.plot(real_y[-display_len:], label='Actual', color='blue', alpha=0.6)
            plt.plot(real_pred[-display_len:], label='Predicted', color='red', linestyle='--', alpha=0.8)
            
            plt.title('LSTM Model Prediction (Recent 100)')
            plt.xlabel('Time Steps')
            plt.ylabel('Price')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ì´ë¯¸ì§€ ì €ì¥
            save_dir = "data/plots"
            if not os.path.exists(save_dir):
                os.makedirs(save_dir)
            
            save_path = os.path.join(save_dir, "lstm_prediction.png")
            plt.savefig(save_path)
            plt.close()
            
            # í…”ë ˆê·¸ë¨ ì „ì†¡
            if TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID:
                url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendPhoto"
                with open(save_path, 'rb') as f:
                    files = {'photo': f}
                    data = {'chat_id': TELEGRAM_CHAT_ID, 'caption': 'ğŸ“Š LSTM í•™ìŠµ ê²°ê³¼ (Actual vs Predicted)'}
                    requests.post(url, data=data, files=files, timeout=5)
                    
        except Exception as e:
            logger.error(f"ì‹œê°í™” ì „ì†¡ ì˜¤ë¥˜: {e}")

    def predict_direction(self, data, current_price):
        """ë‹¤ìŒ ìº”ë“¤ ë°©í–¥ ì˜ˆì¸¡"""
        if not self.is_trained or self.model is None:
            return "HOLD"
            
        try:
            # ë°ì´í„° ì¤€ë¹„ (ë§ˆì§€ë§‰ ìœˆë„ìš°)
            if len(data) < self.lookback_window:
                return "HOLD"
                
            close_prices = data['close'].values.reshape(-1, 1)
            scaled_data = self.scaler.transform(close_prices)
            
            last_window = scaled_data[-self.lookback_window:].reshape(1, -1)
            
            if self.model_type == "lstm":
                last_window = np.reshape(last_window, (1, self.lookback_window, 1))
                predicted_scaled = self.model.predict(last_window, verbose=0)[0][0]
                predicted_price = self.scaler.inverse_transform([[predicted_scaled]])[0][0]
                
                # 0.1% ì´ìƒ ë³€ë™ ì‹œ ë°©í–¥ì„± ì œì‹œ
                if predicted_price > current_price * 1.001:
                    return "UP"
                elif predicted_price < current_price * 0.999:
                    return "DOWN"
                else:
                    return "HOLD"
            else:
                prediction = self.model.predict(last_window)[0]
                return "UP" if prediction == 1 else "DOWN"
                
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {e}")
            return "HOLD"

    def save_model(self, path):
        if not self.is_trained: return
        try:
            if self.model_type == "lstm":
                self.model.save(path.replace(".pkl", ".h5"))
                joblib.dump(self.scaler, path.replace(".pkl", "_scaler.pkl"))
            else:
                joblib.dump(self.model, path)
                joblib.dump(self.scaler, path.replace(".pkl", "_scaler.pkl"))
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì˜¤ë¥˜: {e}")